from fastapi import FastAPI, HTTPException # ‚¨ÖÔ∏è TH√äM import FastAPI
from pydantic import BaseModel # ‚¨ÖÔ∏è TH√äM import BaseModel
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings
from fuzzywuzzy import fuzz
from unidecode import unidecode
import pandas as pd
import json, os, re, sys


# ========================================
# 1Ô∏è‚É£ KH·ªûI T·∫†O & BI·∫æN TO√ÄN C·ª§C
# ========================================
app = FastAPI(title="Chatbot HCMUE RAG API", version="2.2") 
SYSTEM_STATUS = "OK" # ‚¨ÖÔ∏è KHAI B√ÅO BI·∫æN TR·∫†NG TH√ÅI H·ªÜ TH·ªêNG

# ========================================
# 2Ô∏è‚É£ H√ÄM H·ªñ TR·ª¢ (GI·ªÆ NGUY√äN)
def remove_vietnamese_diacritics(text: str) -> str:
    """Lo·∫°i b·ªè d·∫•u ti·∫øng Vi·ªát v√† chuy·ªÉn sang ch·ªØ th∆∞·ªùng."""
    return unidecode(text).lower()

def pretty(text: str) -> str:
    """ƒê·ªãnh d·∫°ng vƒÉn b·∫£n d·ªÖ ƒë·ªçc h∆°n."""
    text = re.sub(r"([;:.\)\]\}])\s*(?=[\+\-‚Ä¢‚Äì])", r"\1\n", text)
    text = re.sub(r"\s*([+\-‚Ä¢‚Äì])\s+", r"\n\1 ", text)
    text = re.sub(r"\s*(ƒêi·ªÅu\s+\d+\.)", r"\n\1", text, flags=re.IGNORECASE)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()

def classify_query_intent(query: str) -> str:
    """Ph√¢n lo·∫°i truy v·∫•n l√† v·ªÅ M√¥n h·ªçc hay S·ªï tay."""
    normalized_query = remove_vietnamese_diacritics(query)
    course_phrases = [
        "lap trinh co ban", "co so toan", "toan roi rac", "thiet ke web",
        "duong loi quoc phong", "phap luat dai cuong", "triet hoc mac lenin",
        "tam ly hoc", "giao duc the chat", "lap trinh nang cao",
        "lap trinh huong doi tuong", "cong tac quoc phong", "kinh te chinh tri",
        "chu nghia xa hoi", "phuong phap nghien cuu khoa hoc", "giao duc doi song",
        "phuong phap hoc tap", "ky nang thich ung", "ky nang lam viec nhom",
        "cau truc du lieu", "co so du lieu", "lap trinh tren windows",
        "xac suat thong ke", "ly thuyet do thi", "quan su chung",
        "tu tuong ho chi minh", "kien truc may tinh", "nhap mon mang may tinh",
        "he dieu hanh", "phan tich va thiet ke giai thuat", "quy hoach tuyen tinh",
        "ky thuat chien dau", "lich su dang cong san", "nhap mon cong nghe phan mem",
        "phan tich thiet ke huong doi tuong", "tri tue nhan tao", "cac he co so du lieu",
        "thiet ke va quan ly mang lan", "phan tich va thiet ke he thong thong tin",
        "co so du lieu nang cao", "he thong ma nguon mo", "xu ly anh so",
        "quan tri co ban voi windows server", "nghi thuc giao tiep mang",
        "phat trien ung dung tren thiet bi di dong", "quan ly du an cong nghe thong tin",
        "kiem thu phan mem", "phat trien ung dung tro cho choi",
        "quy trinh phat trien phan mem agile", "he thong nhung", "hoc may",
        "lap trinh python", "lap trinh php", "thuc hanh nghe nghiep",
        "mang may tinh nang cao", "cong nghe web", "cong nghe java",
        "cac he co so tri thuc", "do hoa may tinh", "bao mat va an ninh mang",
        "logic mo", "cong nghe net", "chuyen de oracle", "truyen thong ky thuat so",
        "chuan doan va quan ly su co mang", "dinh tuyen mang nang cao",
        "quan tri mang voi linux", "quan tri dich vu mang",
        "he thong quan tri doanh nghiep", "xay dung du an cong nghe thong tin",
        "he tu van thong tin", "bao mat co so du lieu", "khai thac du lieu va ung dung",
        "lap trinh tien hoa", "cac phuong phap hoc thong ke",
        "lap rap cai dat va bao tri may tinh", "internet van vat", "nhap mon devops",
        "cong nghe chuoi khoi", "cac giai thuat tinh toan dai so",
        "khai thac du lieu van ban", "xu ly ngon ngu tu nhien", "ly thuyet ma hoa va mat ma",
        "thuc tap nghe nghiep", "khoi nghiep", "cong nghe phan mem nang cao",
        "cong nghe mang khong day", "thuong mai dien tu", "kiem thu phan mem nang cao",
        "dien toan dam may", "do hoa may tinh nang cao", "phan tich du lieu",
        "may hoc nang cao", "thi giac may tinh", "phan tich anh y khoa",
        "phat trien ung dung tren thiet bi di dong nang cao", "khoa luan tot nghiep",
        "ho so tot nghiep", "san pham nghien cuu",
        # Th√™m c√°c t·ª´ kh√≥a m√¥ t·∫£ √Ω ƒë·ªãnh
        "mo ta mon", "thong tin mon", "hoc phan", "mon hoc", "hoc gi"
    ]

    strong_single_keywords = [
        "mon", "hoc phan", "lap trinh", "toan", "python", "java", "web",
        "du lieu", "ai", "linux", "windows", "thong ke", "do hoa", "bao mat",
        "mang", "phap luat", "triet hoc", "tam ly", "lich su", "kinh te",
        "phat trien", "thiet ke", "quantri", "server", "oracle", "devops",
        "agile", "khoi nghiep", "vat ly", "hoa hoc", "su pham", "tin hoc",
        "huong doi tuong"
    ]

    if any(phrase in normalized_query for phrase in course_phrases):
        return "COURSE"

    if len(normalized_query.split()) <= 4 and any(k in normalized_query for k in strong_single_keywords):
        return "COURSE"

    return "GENERAL"


# ========================================
# 3Ô∏è‚É£ C·∫§U H√åNH & T·∫¢I M√î H√åNH
# ========================================
DB_DIR = "./vector_store"
COLLECTION = "so_tay_hcmue"
TABLE_JSON = "./so_tay_all_tables_clean.json"
COURSE_JSON = "./mon_hoc_mo_ta.json"
MODEL_NAME = "BAAI/bge-m3"

try:
    print("üöÄ ƒêang t·∫£i m√¥ h√¨nh...")
    model = SentenceTransformer(MODEL_NAME)
    import torch
    torch.cuda.empty_cache()
    
    chromadb.api.client.SharedSystemClient._instance = None
    client = chromadb.PersistentClient(path=DB_DIR, settings=Settings())
    col = None
    
    if COLLECTION in [c.name for c in client.list_collections()]:
        col = client.get_collection(COLLECTION)
    else:
        col = client.create_collection(COLLECTION)

    # ‚¨áÔ∏è‚¨áÔ∏è LOGIC T·ª∞ ƒê·ªòNG INDEXING (N·∫æU DB TR·ªêNG) ‚¨áÔ∏è‚¨áÔ∏è
    if col.count() == 0:
        print("‚ö†Ô∏è Collection tr·ªëng. ƒêang ki·ªÉm tra d·ªØ li·ªáu v√† Indexing...")
        CHUNKS_PATH = "./chunks.txt" 
        if os.path.exists(CHUNKS_PATH):
            with open(CHUNKS_PATH, 'r', encoding='utf-8') as f:
                data = f.read().split('\n\n')
            
            documents = [chunk.strip() for chunk in data if chunk.strip()]
            
            if documents:
                metadatas = [{"source": "So_Tay_Chinh", "chunk_id": i} for i in range(len(documents))]
                ids = [f"doc_{i}" for i in range(len(documents))]
                
                print(f"B·∫Øt ƒë·∫ßu Indexing {len(documents)} t√†i li·ªáu...")
                embeddings = model.encode(documents, normalize_embeddings=True).tolist() 
                
                col.add(
                    embeddings=embeddings,
                    documents=documents,
                    metadatas=metadatas,
                    ids=ids
                )
                print("‚úÖ Indexing ho√†n t·∫•t. Vector Database ƒë√£ s·∫µn s√†ng.")
                torch.cuda.empty_cache()
            else:
                SYSTEM_STATUS = "CHUNK_FILE_EMPTY"
                print("‚ùå T·ªáp chunks.txt r·ªóng. Kh√¥ng th·ªÉ t·∫°o Vector DB.")
        else:
            SYSTEM_STATUS = "CHUNK_FILE_MISSING"
            print(f"‚ùå KH√îNG T√åM TH·∫§Y T·ªÜP CHUNKS {CHUNKS_PATH}. Kh√¥ng th·ªÉ t·∫°o Vector DB.")
    # ‚¨ÜÔ∏è‚¨ÜÔ∏è H·∫æT LOGIC T·ª∞ ƒê·ªòNG INDEXING ‚¨ÜÔ∏è‚¨ÜÔ∏è

except Exception as e:
    SYSTEM_STATUS = "INIT_ERROR"
    print(f"‚ùå L·ªói kh·ªüi t·∫°o h·ªá th·ªëng: {e}")
    # ƒê·∫∑t c√°c bi·∫øn quan tr·ªçng th√†nh None ƒë·ªÉ tr√°nh l·ªói sau n√†y
    col = None
    model = None


# üåü 4Ô∏è‚É£ T·∫¢I PHOGPT ‚Äì MODEL SINH C√ÇU T·ª∞ NHI√äN
# ========================================
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

try:
    print("ü§ñ Loading Gemma model for natural responses...")
    PHO_MODEL_ID = "google/gemma-2b-it"
    tok = AutoTokenizer.from_pretrained(PHO_MODEL_ID)
    mod = AutoModelForCausalLM.from_pretrained(PHO_MODEL_ID)
    llm = pipeline(
        "text-generation",
        model=mod,
        tokenizer=tok,
        device_map="auto",
        torch_dtype="auto",      # ‚úÖ th√™m d√≤ng n√†y
        do_sample=False,
        max_new_tokens=180,      # ‚úÖ GI·∫¢M XU·ªêNG
        temperature=0.3,
        top_p=0.9
    )
    print("‚úÖ Gemma ready.")
except Exception as e:
    SYSTEM_STATUS = "LLM_LOAD_ERROR"
    llm = None
    print(f"‚ùå Error loading Gemma: {e}")

# ========================================
# 5Ô∏è‚É£ T·∫¢I D·ªÆ LI·ªÜU JSON
# ========================================

def load_json(path):
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

tables = load_json(TABLE_JSON)
courses = load_json(COURSE_JSON)
COURSE_DATA = {
    remove_vietnamese_diacritics(c["ten_mon"]): {
        "ten_mon": c["ten_mon"],
        "Description": c["Description"]
    }
    for c in courses
}

COURSE_STOPWORDS = {
    "mon", "monhoc", "hoc", "hocphan", "phan", "mo", "ta", "mota",
    "noi", "dung", "noidung", "la", "gi"
}

COURSE_ALIASES = {
    remove_vietnamese_diacritics("Gi√°o d·ª•c th·ªÉ ch·∫•t 1"): remove_vietnamese_diacritics("Gi√°o d·ª•c th·ªÉ ch·∫•t 1 (Th·ªÉ d·ª•c - ƒêi·ªÅn kinh)"),
    remove_vietnamese_diacritics("K·ªπ nƒÉng l√†m vi·ªác nh√≥m"): remove_vietnamese_diacritics("K·ªπ nƒÉng l√†m vi·ªác nh√≥m v√† t∆∞ duy s√°ng t·∫°o"),
    remove_vietnamese_diacritics("L·∫≠p tr√¨nh tr√™n thi·∫øt b·ªã di ƒë·ªông"): remove_vietnamese_diacritics("Ph√°t tri·ªÉn ·ª©ng d·ª•ng tr√™n thi·∫øt b·ªã di ƒë·ªông"),
    remove_vietnamese_diacritics("L·∫≠p tr√¨nh tr√™n thi·∫øt b·ªã di ƒë·ªông n√¢ng cao"): remove_vietnamese_diacritics("Ph√°t tri·ªÉn ·ª©ng d·ª•ng tr√™n thi·∫øt b·ªã di ƒë·ªông n√¢ng cao"),
    remove_vietnamese_diacritics("Ki·∫øn tr√∫c m√°y t√≠nh"): remove_vietnamese_diacritics("Ki·∫øn tr√∫c m√°y t√≠nh v√† h·ª£p ng·ªØ"),
    remove_vietnamese_diacritics("Ki·∫øn tr√∫c m√°y t√≠nh v√† h·ª£p ng·ªØ"): remove_vietnamese_diacritics("Ki·∫øn tr√∫c m√°y t√≠nh v√† h·ª£p ng·ªØ"),
    remove_vietnamese_diacritics("L·∫≠p tr√¨nh windows"): remove_vietnamese_diacritics("L·∫≠p tr√¨nh tr√™n Windows"),
    remove_vietnamese_diacritics("L·∫≠p tr√¨nh window"): remove_vietnamese_diacritics("L·∫≠p tr√¨nh tr√™n Windows"),
    remove_vietnamese_diacritics("L·∫≠p tr√¨nh tr√™n window"): remove_vietnamese_diacritics("L·∫≠p tr√¨nh tr√™n Windows"),
}

for alias_key, canonical_key in COURSE_ALIASES.items():
    canonical_course = COURSE_DATA.get(canonical_key)
    if canonical_course:
        COURSE_DATA.setdefault(alias_key, canonical_course)


def tokenize_course_key(text: str):
    raw_tokens = re.findall(r"[a-z0-9]+", text)
    seen = set()
    tokens = []
    for tok in raw_tokens:
        if not tok or tok in COURSE_STOPWORDS or tok in seen:
            continue
        tokens.append(tok)
        seen.add(tok)
    return tokens


COURSE_TOKENS = {k: tokenize_course_key(k) for k in COURSE_DATA}


def build_grade_lookup(table_data):
    out = {}
    for t in table_data:
        t_type = t.get("type")
        if t_type == "thang_diem_4":
            for row in t.get("data", []):
                key = str(row.get("Thang ƒëi·ªÉm ch·ªØ", "")).strip().upper()
                if not key:
                    continue
                out.setdefault(key, {})["thang_4"] = row.get("Thang ƒëi·ªÉm 4")
        elif t_type == "thang_diem_10_chu":
            for row in t.get("data", []):
                key = str(row.get("Thang ƒëi·ªÉm ch·ªØ", "")).strip().upper()
                if not key:
                    continue
                out.setdefault(key, {})["thang_10"] = row.get("Thang ƒëi·ªÉm 10")
    return out


GRADE_LOOKUP = build_grade_lookup(tables)


def parse_score_range(range_text: str):
    if not range_text:
        return None, None
    cleaned = unidecode(range_text).replace(",", ".")
    numbers = [
        float(num)
        for num in re.findall(r"\d+(?:\.\d+)?", cleaned)
    ]
    if not numbers:
        return None, None
    if len(numbers) == 1:
        return numbers[0], numbers[0]
    return min(numbers), max(numbers)


def build_grade_scale_lookups(table_data):
    ranges_10 = {}
    values_4 = {}
    for t in table_data:
        t_type = t.get("type")
        if t_type == "thang_diem_10_chu":
            for row in t.get("data", []):
                letter = str(row.get("Thang ƒëi·ªÉm ch·ªØ", "")).strip().upper()
                low, high = parse_score_range(row.get("Thang ƒëi·ªÉm 10"))
                if letter and low is not None:
                    ranges_10[letter] = (low, high if high is not None else low)
        elif t_type == "thang_diem_4":
            for row in t.get("data", []):
                letter = str(row.get("Thang ƒëi·ªÉm ch·ªØ", "")).strip().upper()
                val_text = str(row.get("Thang ƒëi·ªÉm 4", "")).strip()
                if not letter or not val_text:
                    continue
                try:
                    value = float(val_text.replace(",", "."))
                except ValueError:
                    continue
                values_4[letter] = value
    return ranges_10, values_4


GRADE_10_RANGES, GRADE_4_VALUES = build_grade_scale_lookups(tables)

PASS_THRESHOLD_10 = 4.0
PASS_THRESHOLD_4 = 1.0


def build_academic_class_lookup(table_data):
    lookup = {}
    for t in table_data:
        if t.get("type") == "xep_loai_hoc_luc":
            for row in t.get("data", []):
                label = str(row.get("X·∫øp lo·∫°i", "")).strip()
                if not label:
                    continue
                key = remove_vietnamese_diacritics(label)
                lookup[key] = row
    return lookup


ACADEMIC_CLASS_LOOKUP = build_academic_class_lookup(tables)

PASSING_GRADES = {"A", "B+", "B", "C+", "C", "D+", "D"}
FAILING_GRADES = {"F+", "F"}
PASSING_GRADE_ORDER = ["A", "B+", "B", "C+", "C", "D+", "D"]
FAILING_GRADE_ORDER = ["F+", "F"]
PASSING_GRADE_ASC_ORDER = ["D", "D+", "C", "C+", "B", "B+", "A"]


def grade_letter_from_score(score: float, scale: int = 10):
    if score is None:
        return None
    if scale == 10:
        for letter, (low, high) in GRADE_10_RANGES.items():
            if low is None:
                continue
            upper = high if high is not None else low
            if low - 1e-6 <= score <= upper + 1e-6:
                return letter
    elif scale == 4:
        best_letter, best_diff = None, float("inf")
        for letter, value in GRADE_4_VALUES.items():
            diff = abs(value - score)
            if diff < best_diff:
                best_diff = diff
                best_letter = letter
        if best_letter is not None and best_diff <= 0.51:  # tolerate rounding on scale 4
            return best_letter
    return None


def is_passing_letter(letter: str):
    if not letter:
        return False
    return letter in PASSING_GRADES


def is_passing_score(score: float, scale: int = 10):
    if score is None:
        return False
    if scale == 10:
        return score >= PASS_THRESHOLD_10 - 1e-6
    if scale == 4:
        return score >= PASS_THRESHOLD_4 - 1e-6
    return False


def extract_numeric_scores(normalized_q: str):
    if not normalized_q:
        return []
    normalized = normalized_q.replace(",", ".")
    values = []
    for match in re.finditer(r"\d+(?:\.\d+)?", normalized):
        start = match.start()
        snippet_start = max(0, start - 15)
        prefix = normalized[snippet_start:start].strip()
        if prefix.endswith("thang") or prefix.endswith("thang diem"):
            # skip the scale value (v√≠ d·ª•: thang 10)
            continue
        try:
            values.append(float(match.group()))
        except ValueError:
            continue
    return values


PASS_QUERY_BLOCKERS = ["xep loai", "gioi", "kha", "trung binh", "xuat sac", "yeu", "kem"]
PASS_QUERY_KEYWORDS = [
    "qua mon", "dat mon", "dat hoc phan", "dat tin chi", "dat khong", "dat duoc", "du dieu kien", "qua duoc mon"
]


def is_general_pass_requirement_query(normalized_q: str):
    if not normalized_q:
        return False
    if any(word in normalized_q for word in PASS_QUERY_BLOCKERS):
        return False
    if "xep loai" in normalized_q:
        return False
    if "diem" not in normalized_q:
        return False
    if any(keyword in normalized_q for keyword in PASS_QUERY_KEYWORDS):
        return True
    if "bao nhieu diem" in normalized_q and "dat" in normalized_q:
        return True
    if re.search(r"diem\s+\d", normalized_q) and "qua" in normalized_q:
        return True
    return False


def format_score_text(value: float):
    if value is None:
        return ""
    return f"{value:.1f}".replace(".", ",")


def handle_pass_requirement_query(question: str, normalized_q: str):
    numeric_scores = extract_numeric_scores(normalized_q)
    if not numeric_scores:
        return {
            "type": "natural_table",
            "natural_answer": (
                "Theo s·ªï tay: ƒê·ªÉ qua m√¥n, sinh vi√™n c·∫ßn ƒë·∫°t t·ªëi thi·ªÉu 4,0 ƒëi·ªÉm thang 10 (t∆∞∆°ng ƒë∆∞∆°ng ƒëi·ªÉm ch·ªØ D) "
                "‚Äì t∆∞∆°ng ·ª©ng 1,0 ƒëi·ªÉm thang 4 tr·ªü l√™n."
            ),
        }

    score = numeric_scores[0]
    prefers_thang10 = ("thang diem 10" in normalized_q) or ("thang 10" in normalized_q) or score > 4
    prefers_thang4 = ("thang diem 4" in normalized_q) or ("thang 4" in normalized_q) or (score <= 4 and not prefers_thang10)

    if prefers_thang10:
        letter = grade_letter_from_score(score, 10)
        passing = is_passing_score(score, 10) or is_passing_letter(letter)
        letter_text = f" t∆∞∆°ng ƒë∆∞∆°ng ƒëi·ªÉm ch·ªØ {letter}" if letter else ""
        status_text = "ƒë∆∞·ª£c xem l√† ƒë·∫°t v√¨ ƒë·∫°t m·ª©c t·ªëi thi·ªÉu 4,0" if passing else "ch∆∞a ƒë·ªß ƒëi·ªÅu ki·ªán v√¨ d∆∞·ªõi m·ª©c 4,0"
        return {
            "type": "natural_table",
            "natural_answer": (
                f"Theo s·ªï tay: {format_score_text(score)} ƒëi·ªÉm thang 10{letter_text}, {status_text}."
            ),
        }

    if prefers_thang4:
        letter = grade_letter_from_score(score, 4)
        passing = is_passing_score(score, 4) or is_passing_letter(letter)
        letter_text = f" t∆∞∆°ng ƒë∆∞∆°ng ƒëi·ªÉm ch·ªØ {letter}" if letter else ""
        status_text = "ƒë∆∞·ª£c xem l√† ƒë·∫°t v√¨ ƒë·∫°t m·ª©c t·ªëi thi·ªÉu 1,0" if passing else "ch∆∞a ƒë·ªß ƒëi·ªÅu ki·ªán v√¨ d∆∞·ªõi m·ª©c 1,0"
        return {
            "type": "natural_table",
            "natural_answer": (
                f"Theo s·ªï tay: {format_score_text(score)} ƒëi·ªÉm thang 4{letter_text}, {status_text}."
            ),
        }

    # fallback chung n·∫øu kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c thang
    passing = is_passing_score(score, 10) or is_passing_score(score, 4)
    status_text = "ƒë∆∞·ª£c xem l√† ƒë·∫°t" if passing else "ch∆∞a ƒë·ªß ƒëi·ªÅu ki·ªán ƒë·ªÉ qua m√¥n"
    return {
        "type": "natural_table",
        "natural_answer": (
            f"Theo s·ªï tay: ƒêi·ªÉm {format_score_text(score)} {status_text}. M·ª©c t·ªëi thi·ªÉu l√† 4,0 thang 10 ho·∫∑c 1,0 thang 4."
        ),
    }


def build_scholarship_lookup(table_data):
    lookup = {}
    current_key = None
    current_label = None
    for t in table_data:
        if t.get("type") != "yeu_cau_hoc_bong":
            continue
        for row in t.get("data", []):
            raw_label = str(row.get("Lo·∫°i h·ªçc b·ªïng", "")).strip()
            if raw_label:
                current_label = raw_label
                current_key = remove_vietnamese_diacritics(raw_label)
                lookup.setdefault(current_key, {"label": raw_label, "combos": []})
            if not current_key:
                continue
            academic = str(row.get("Y√™u c·∫ßu", "")).strip()
            conduct = str(row.get("", "")).strip()
            if not academic and not conduct:
                continue
            # B·ªè qua d√≤ng ti√™u ƒë·ªÅ ph·ª•
            if academic.lower().startswith("k·∫øt qu·∫£"):
                continue
            lookup[current_key]["combos"].append({"academic": academic, "conduct": conduct})
    return lookup


def parse_training_range(range_text: str):
    if not range_text:
        return None, None, True
    normalized_text = unidecode(range_text).lower()
    cleaned_numbers = range_text.replace(",", ".")
    numbers = [float(num.replace(",", ".")) for num in re.findall(r"\d+(?:[.,]\d+)?", cleaned_numbers)]
    min_score, max_score = None, None
    max_inclusive = True
    if "tu" in normalized_text:
        if numbers:
            min_score = numbers[0]
        if "den duoi" in normalized_text:
            if len(numbers) >= 2:
                max_score = numbers[1]
                max_inclusive = False
        elif "den" in normalized_text:
            if len(numbers) >= 2:
                max_score = numbers[1]
                max_inclusive = True
    elif normalized_text.strip().startswith("duoi"):
        if numbers:
            max_score = numbers[0]
            max_inclusive = False
            min_score = 0.0
    elif normalized_text.strip().startswith("tren"):
        if numbers:
            min_score = numbers[0]
            max_score = None
            max_inclusive = True
    return min_score, max_score, max_inclusive


def build_training_rank_lookup(table_data):
    ranks = {}
    for t in table_data:
        if t.get("type") != "xep_loai_hoc_bong":
            continue
        for row in t.get("data", []):
            label = str(row.get("X·∫øp lo·∫°i", "")).strip()
            range_text = str(row.get("Khung ƒëi·ªÉm", "")).strip()
            if not label or not range_text:
                continue
            key = remove_vietnamese_diacritics(label)
            min_score, max_score, max_inclusive = parse_training_range(range_text)
            ranks[key] = {
                "label": label,
                "range": range_text,
                "min_score": min_score,
                "max_score": max_score,
                "max_inclusive": max_inclusive,
            }
    return ranks


SCHOLARSHIP_LOOKUP = build_scholarship_lookup(tables)
TRAINING_RANK_LOOKUP = build_training_rank_lookup(tables)


def determine_training_rank(score: float):
    if score is None:
        return None
    for info in TRAINING_RANK_LOOKUP.values():
        min_score = info.get("min_score")
        max_score = info.get("max_score")
        max_inclusive = info.get("max_inclusive", True)
        if min_score is not None and score + 1e-9 < min_score:
            continue
        if max_score is None:
            return info
        if max_inclusive:
            if score <= max_score + 1e-9:
                return info
        else:
            if score + 1e-9 < max_score:
                return info
    return None

GENERAL_SCHOLARSHIP_CONDITIONS = (
    "Theo s·ªï tay: ƒêi·ªÅu ki·ªán ƒë·ªÉ ƒë∆∞·ª£c x√©t h·ªçc b·ªïng khuy·∫øn kh√≠ch h·ªçc t·∫≠p g·ªìm:\n"
    "- L√† sinh vi√™n h·ªá ch√≠nh quy ƒë√∫ng ti·∫øn ƒë·ªô kho√° h·ªçc (kh√¥ng x√©t cho sinh vi√™n qu√° th·ªùi gian h·ªçc t·∫≠p chu·∫©n).\n"
    "- K·∫øt qu·∫£ h·ªçc t·∫≠p v√† r√®n luy·ªán c·ªßa h·ªçc k·ª≥ ƒë·∫°t t·ª´ lo·∫°i Kh√° tr·ªü l√™n, kh√¥ng b·ªã k·ª∑ lu·∫≠t t·ª´ m·ª©c khi·ªÉn tr√°ch tr·ªü l√™n.\n"
    "- T√≠ch l≈©y t·ªëi thi·ªÉu 15 t√≠n ch·ªâ trong h·ªçc k·ª≥ x√©t h·ªçc b·ªïng (kh√¥ng t√≠nh t√≠n ch·ªâ tr·∫£ n·ª£, c·∫£i thi·ªán ho·∫∑c t∆∞∆°ng ƒë∆∞∆°ng).\n"
    "- T·∫•t c·∫£ t√≠n ch·ªâ ƒëƒÉng k√Ω trong h·ªçc k·ª≥ ph·∫£i ƒë·∫°t (kh√¥ng n·ª£ m√¥n).\n"
    "- H·ªçc k·ª≥ cu·ªëi: kho√° 2022 tr·ªü ƒëi t√≠ch l≈©y t·ª´ 11 t√≠n ch·ªâ; kho√° 2021 tr·ªü v·ªÅ tr∆∞·ªõc t√≠ch l≈©y t·ª´ 6 t√≠n ch·ªâ."
)


def parse_min_bound(range_text: str):
    if not range_text:
        return None
    cleaned = range_text.replace(",", ".")
    numbers = re.findall(r"\d+(?:\.\d+)?", cleaned)
    if numbers:
        try:
            return float(numbers[0])
        except ValueError:
            return None
    if "d∆∞·ªõi" in range_text.lower():
        return 0.0
    return None


ACADEMIC_MIN_POINTS = {
    key: parse_min_bound(row.get("Thang ƒëi·ªÉm 4"))
    for key, row in ACADEMIC_CLASS_LOOKUP.items()
}

TRAINING_MIN_POINTS = {
    key: parse_min_bound(info.get("range"))
    for key, info in TRAINING_RANK_LOOKUP.items()
}


def format_decimal(value: float):
    if value is None:
        return None
    if abs(value - int(value)) < 1e-6:
        return str(int(value))
    return f"{value:.1f}".replace(".", ",")


def normalize_label_key(label: str):
    return remove_vietnamese_diacritics(label or "").strip()


def base_label_key(label: str):
    key = normalize_label_key(label)
    if key.endswith(" tro len"):
        key = key.replace(" tro len", "").strip()
    return key


def describe_academic_requirement(label: str):
    if not label:
        return None
    key = normalize_label_key(label)
    min_point = ACADEMIC_MIN_POINTS.get(key)
    min_txt = format_decimal(min_point)
    if min_txt:
        return f"ƒêi·ªÉm h·ªçc t·∫≠p {label.lower()} (GPA thang 4 t·ª´ {min_txt} tr·ªü l√™n)"
    return f"ƒêi·ªÉm h·ªçc t·∫≠p {label.lower()}"


def describe_conduct_requirement(label: str):
    if not label:
        return None
    key = base_label_key(label)
    min_point = TRAINING_MIN_POINTS.get(key)
    min_txt = format_decimal(min_point)
    if min_txt:
        return f"ƒêi·ªÉm r√®n luy·ªán {label.lower()} (t·ª´ {min_txt} ƒëi·ªÉm tr·ªü l√™n)"
    return f"ƒêi·ªÉm r√®n luy·ªán {label.lower()}"


def get_scholarship_level_details(level_key: str):
    entry = SCHOLARSHIP_LOOKUP.get(level_key)
    if not entry:
        return None, []
    combos = []
    for combo in entry.get("combos", []):
        academic_label = combo.get("academic")
        conduct_label = combo.get("conduct")
        academic_key = normalize_label_key(academic_label)
        conduct_key = base_label_key(conduct_label)
        academic_min = ACADEMIC_MIN_POINTS.get(academic_key)
        conduct_min = TRAINING_MIN_POINTS.get(conduct_key)
        combos.append({
            "academic_label": academic_label,
            "academic_min": academic_min,
            "academic_desc": describe_academic_requirement(academic_label),
            "conduct_label": conduct_label,
            "conduct_min": conduct_min,
            "conduct_desc": describe_conduct_requirement(conduct_label),
        })
    return entry.get("label"), combos


SCHOLARSHIP_LEVEL_PATTERNS = {
    "xuat sac": r"xuat\s*sac",
    "gioi": r"\bgioi\b",
    "kha": r"\bkha\b",
    "trung binh": r"trung\s*binh",
}

SCHOLARSHIP_LEVEL_LABELS = {
    "xuat sac": "Xu·∫•t s·∫Øc",
    "gioi": "Gi·ªèi",
    "kha": "Kh√°",
    "trung binh": "Trung b√¨nh",
}


def detect_scholarship_level(normalized_q: str):
    for key, pattern in SCHOLARSHIP_LEVEL_PATTERNS.items():
        if re.search(pattern, normalized_q):
            return key
    return None


def is_scholarship_no_debt_query(normalized_q: str):
    return "hoc bong" in normalized_q and any(kw in normalized_q for kw in ["no mon", "no hoc phan", "no tin chi"])


def is_poor_conduct_query(normalized_q: str):
    return "hoc bong" in normalized_q and "ren luyen" in normalized_q and "trung binh" in normalized_q


def detect_academic_class_key(normalized_q: str):
    if not normalized_q:
        return None
    for key in ACADEMIC_CLASS_LOOKUP.keys():
        if re.search(rf"xep\s+loai\s+{key}\b", normalized_q):
            return key
    if "xep" in normalized_q:
        for key in ACADEMIC_CLASS_LOOKUP.keys():
            if re.search(rf"\bloai\s+{key}\b", normalized_q):
                return key
    return None


def is_minimum_credit_query(normalized_q: str):
    if not normalized_q:
        return False
    if "tin chi" not in normalized_q and "tin" not in normalized_q:
        return False
    return any(phrase in normalized_q for phrase in ["toi thieu bao nhieu", "toi thieu bao nhieu tin", "toi thieu bao nhieu tin chi", "hoc toi thieu", "dang ky toi thieu"]) or (
        "toi thieu" in normalized_q and "tin" in normalized_q
    )

# ========================================
# 6Ô∏è‚É£ TRA C·ª®U B·∫¢NG / M√îN / VECTOR
# ========================================

def find_table_by_keyword(query: str):
    normalized_query = remove_vietnamese_diacritics(query)

    # üö´ N·∫øu ng∆∞·ªùi d√πng h·ªèi "ƒëi·ªÅu ki·ªán x√©t h·ªçc b·ªïng" ‚Üí KH√îNG tra b·∫£ng
    if "hoc bong" in normalized_query and any(kw in normalized_query for kw in ["dieu kien", "tieu chi", "xet", "tieu chuan", "nhan"]):
        return None

    # 3Ô∏è‚É£ N·∫øu kh√¥ng h·ªèi c·ª• th·ªÉ ‚Üí tra b·∫£ng nh∆∞ c≈©
    mapping = {
        "4 sang chu": ["thang_diem_4"],
        "hoc bong": ["xep_loai_hoc_bong"],
        "chu sang 10": ["thang_diem_10_chu"],
        "xep loai hoc luc": ["xep_loai_hoc_luc"],
        "yeu cau hoc bong": ["yeu_cau_hoc_bong"],
        "diem ren luyen": [
            "diem_ren_luyen1", "diem_ren_luyen2",
            "diem_ren_luyen3", "diem_ren_luyen4", "diem_ren_luyen5"
        ]
    }

    best_key, best_score = None, 0
    BEST_MATCH_THRESHOLD = 85

    for k in mapping.keys():
        score = max(
            fuzz.WRatio(normalized_query, k),
            fuzz.partial_ratio(normalized_query, k),
            fuzz.token_set_ratio(normalized_query, k)
        )
        if score > best_score and score >= BEST_MATCH_THRESHOLD:
            best_score = score
            best_key = k

    if not best_key:
        return None

    out = [f"### üìä K·∫øt qu·∫£ tra c·ª©u B·∫£ng (ƒê·ªô kh·ªõp: {best_score}%)"]
    for type_name in mapping[best_key]:
        for t in tables:
            if t.get("type") == type_name:
                df = pd.DataFrame(t["data"])
                title = t.get("title", type_name.replace("_", " ").title())
                out.append(f"#### {title}\n{df.to_html(index=False)}")
    return "\n".join(out) if len(out) > 1 else None



def find_course_by_fuzzy_match(query: str):
    qn = remove_vietnamese_diacritics(query)
    token_set = set(tokenize_course_key(qn))

    if token_set:
        token_matches = []
        for key, tokens in COURSE_TOKENS.items():
            token_len = len(tokens)
            if token_len < 2:
                continue
            if all(tok in token_set for tok in tokens):
                token_matches.append((len(tokens), COURSE_DATA[key]))
        if token_matches:
            token_matches.sort(reverse=True, key=lambda item: item[0])
            best_course = token_matches[0][1]
            return {
                "ten_mon": best_course["ten_mon"],
                "description": best_course["Description"],
                "match_score": 100,
            }

        partial_best_course = None
        partial_best_metrics = (-1.0, -1, -1)
        for key, tokens in COURSE_TOKENS.items():
            token_len = len(tokens)
            if token_len < 2:
                continue
            overlap = token_set.intersection(tokens)
            if not overlap:
                continue
            token_count = token_len
            overlap_size = len(overlap)
            coverage = overlap_size / token_count if token_count else 0
            required_overlap = 3 if token_count >= 5 else 2
            if token_count >= 8:
                coverage_threshold = 0.45
            elif token_count >= 5:
                coverage_threshold = 0.5
            else:
                coverage_threshold = 0.6
            if overlap_size < required_overlap or coverage < coverage_threshold:
                continue
            metrics = (coverage, overlap_size, -token_count)
            if metrics > partial_best_metrics:
                partial_best_metrics = metrics
                partial_best_course = COURSE_DATA[key]
        if partial_best_course:
            coverage = partial_best_metrics[0]
            best_course = partial_best_course
            return {
                "ten_mon": best_course["ten_mon"],
                "description": best_course["Description"],
                "match_score": int(coverage * 100),
            }

    direct_matches = []
    for key, course in COURSE_DATA.items():
        if key and (key in qn or qn in key):
            direct_matches.append((len(key), course))
    if direct_matches:
        direct_matches.sort(reverse=True, key=lambda item: item[0])
        best_course = direct_matches[0][1]
        return {
            "ten_mon": best_course["ten_mon"],
            "description": best_course["Description"],
            "match_score": 100,
        }

    best_score, best_course = 0, None
    for key, course in COURSE_DATA.items():
        score = max(
            fuzz.token_set_ratio(qn, key),
            fuzz.WRatio(qn, key),
            fuzz.partial_ratio(qn, key)
        )
        if score > best_score:
            best_score, best_course = score, course
    if best_course and best_score >= 80:
        return {
            "ten_mon": best_course["ten_mon"],
            "description": best_course["Description"],
            "match_score": best_score,
        }
    return None

def enrich_query_with_keywords(query: str) -> str:
    q_lower = query.lower()
    if "h·ªçc b·ªïng" in q_lower or "hoc bong" in q_lower:
        query += " ƒëi·ªÅu ki·ªán h·ªçc b·ªïng, quy ƒë·ªãnh h·ªçc b·ªïng, ti√™u ch√≠ h·ªçc b·ªïng"
    elif "t·ªët nghi·ªáp" in q_lower or "tot nghiep" in q_lower:
        query += (
            " c√¥ng nh·∫≠n t·ªët nghi·ªáp, x√©t t·ªët nghi·ªáp, ƒëi·ªÅu ki·ªán x√©t t·ªët nghi·ªáp, quy ƒë·ªãnh c√¥ng nh·∫≠n t·ªët nghi·ªáp, "
            "ho√†n th√†nh ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o, ƒë·ªß t√≠n ch·ªâ, ƒëi·ªÉm r√®n luy·ªán, chu·∫©n ƒë·∫ßu ra, h·ªçc ph·∫ßn b·∫Øt bu·ªôc, h·ªçc ph·∫ßn t·ª± ch·ªçn"
        )
    elif "r√®n luy·ªán" in q_lower or "ren luyen" in q_lower:
        query += " ti√™u ch√≠ ƒëi·ªÉm r√®n luy·ªán, ƒëi·ªÅu ki·ªán ƒëi·ªÉm r√®n luy·ªán, x·∫øp lo·∫°i ƒëi·ªÉm r√®n luy·ªán"
    return query



def vector_search(query: str, top_k=1):
    if col is None or model is None:
        raise Exception("Vector DB ho·∫∑c Model ch∆∞a kh·ªüi t·∫°o.")
    
    q_emb = model.encode(enrich_query_with_keywords(query), normalize_embeddings=True).tolist()
    res = col.query(
        query_embeddings=[q_emb],
        include=["documents", "metadatas", "distances"],  # th√™m distances
        where={"source": {"$eq": "So_Tay_Chinh"}},
        n_results=top_k,
    )

    docs = (res.get("documents") or [[]])[0]
    metas = (res.get("metadatas") or [[]])[0]
    dists = (res.get("distances") or [[]])[0]

    out = []
    for i, d in enumerate(docs):
        if d and d.strip():
            out.append({
                "chunk": pretty(d),
                "source": (metas[i] or {}).get("source", "So_Tay"),
                "distance": dists[i] if i < len(dists) else None
            })
    return out


# ===== Helper b·∫Øt bu·ªôc: ƒë·∫∑t g·∫ßn generate_natural_answer =====
import re

TIME_PAT = r"(\d{1,2})g(\d{2})"
PERIOD_LINE_PAT = re.compile(
    rf"Ti·∫øt\s*(\d+)\s*:\s*t·ª´\s*{TIME_PAT}\s*ƒë·∫øn\s*{TIME_PAT}",
    flags=re.IGNORECASE
)

def extract_period_times(ctx: str):
    """
    Qu√©t context ƒë·ªÉ gom gi·ªù c·ªßa t·ª´ng 'Ti·∫øt X'.
    Tr·∫£ v·ªÅ: { int: {'start':'10g40','end':'11g30'} }
    """
    out = {}
    for m in PERIOD_LINE_PAT.finditer(ctx or ""):
        period = int(m.group(1))
        start = f"{m.group(2)}g{m.group(3)}"
        end   = f"{m.group(4)}g{m.group(5)}"
        out[period] = {"start": start, "end": end}
    return out

# ===== MINI-CACHE cho gi·ªù h·ªçc =====
_period_cache = {}


def parse_time_label(label: str):
    match = re.match(r"^(\d{1,2})g(\d{2})$", label or "")
    if not match:
        return None
    hour = int(match.group(1))
    minute = int(match.group(2))
    return hour * 60 + minute


def format_duration(minutes: int):
    if minutes is None or minutes < 0:
        return None
    hours = minutes // 60
    mins = minutes % 60
    parts = []
    if hours:
        parts.append(f"{hours} gi·ªù")
    if mins:
        parts.append(f"{mins} ph√∫t")
    return " ".join(parts) if parts else "0 ph√∫t"

def get_period_times():
    """
    L·∫•y gi·ªù t·ª´ng ti·∫øt t·ª´ cache; n·∫øu ch∆∞a c√≥ th√¨ ƒë·ªçc t·ª´ chunks ho·∫∑c query vector 1 l·∫ßn.
    """
    global _period_cache
    if _period_cache:
        return _period_cache
    try:
        # ∆Øu ti√™n l·∫•y t·ª´ file chunks.txt (n·∫øu c√≥)
        if os.path.exists("./chunks.txt"):
            with open("./chunks.txt", "r", encoding="utf-8") as f:
                raw = f.read()
            _period_cache = extract_period_times(raw)
        else:
            # fallback: query vector
            res = vector_search("th·ªùi kh√≥a bi·ªÉu", top_k=3)
            ctx = "\n".join([r["chunk"] for r in res]) if res else ""
            _period_cache = extract_period_times(ctx)
    except Exception:
        _period_cache = {}
    return _period_cache


def compute_break_duration(period_a: int, period_b: int):
    periods = get_period_times()
    if not periods:
        return None
    data_a = periods.get(period_a)
    data_b = periods.get(period_b)
    if not data_a or not data_b:
        return None
    end_minutes = parse_time_label(data_a.get("end"))
    start_minutes = parse_time_label(data_b.get("start"))
    if end_minutes is None or start_minutes is None:
        return None
    return start_minutes - end_minutes

def question_targets_period(question: str):
    m = re.search(r"ti·∫øt\s*(\d+)", (question or "").lower())
    return int(m.group(1)) if m else None

def extract_bullets(ctx: str):
    """
    Tr√≠ch t·∫•t c·∫£ d√≤ng bullet c√≥ d·∫°ng -, +, ‚Ä¢, a), 1., 1) ...
    Tr·∫£ v·ªÅ list ƒë√£ lo·∫°i tr√πng.
    """
    lines = []
    for line in (ctx or "").splitlines():
        l = line.strip()
        if not l: 
            continue
        if l.startswith(("-", "+", "‚Ä¢")):
            lines.append(l.lstrip("-+‚Ä¢").strip())
        elif re.match(r"^\(?[a-zA-Z]\)\s+", l):        # a) b) c)
            lines.append(l)
        elif re.match(r"^\d+[\.\)]\s+", l):            # 1. 2) 3.
            lines.append(l)
    # unique (case-insensitive)
    seen, uniq = set(), []
    for l in lines:
        k = l.lower()
        if k not in seen:
            uniq.append(l)
            seen.add(k)
    return uniq

def is_scholarship_condition_query(q: str):
    qn = (q or "").lower()
    # c√°c c·ª•m ph·ªï bi·∫øn khi h·ªèi ƒëi·ªÅu ki·ªán x√©t h·ªçc b·ªïng
    return ("h·ªçc b·ªïng" in qn or "hoc bong" in qn) and any(k in qn for k in [
        "ƒëi·ªÅu ki·ªán", "dieu kien", "ti√™u chu·∫©n", "tieu chuan", "x√©t", "xet"
    ])

def has_bullet(text: str):
    return bool(re.search(r"(^\s*[-+‚Ä¢*]\s)|(^\s*\(?[a-zA-Z0-9]+\)|^\s*\d+[\.\)])", text, flags=re.MULTILINE))

def too_short(text: str, min_words: int = 35):
    return len((text or "").split()) < min_words


GRADE_TOKEN_PATTERN = re.compile(r"(?<![A-Z])([A-F](?:\+|-)?) (?=$|[^A-Z0-9])", re.VERBOSE)


def extract_grade_token(text: str):
    if not text:
        return None
    upper_text = unidecode(text).upper()
    for token in ["F+", "D+", "C+", "B+", "A+"]:
        if token in upper_text:
            return token
    tokens = GRADE_TOKEN_PATTERN.findall(upper_text)
    for token in tokens:
        if token in GRADE_LOOKUP or token in PASSING_GRADES or token in FAILING_GRADES:
            return token
    return None
# ===== End helper =====


# 7Ô∏è‚É£ H√ÄM SINH C√ÇU TR·∫¢ L·ªúI T·ª∞ NHI√äN (v5 ‚Äì guardrail H·ªçc b·ªïng + Gi·ªù/ti·∫øt)
# ======================================================================
def generate_natural_answer(question: str, context: str) -> str:
    """
    M·ª•c ti√™u:
    - H·ªèi 'Ti·∫øt X' -> tr·∫£ gi·ªù ch√≠nh x√°c t·ª´ context (regex), tr√°nh sai s·ªë li·ªáu.
    - H·ªèi 'ƒëi·ªÅu ki·ªán h·ªçc b·ªïng' -> li·ªát k√™ ƒê·ª¶ √Ω t·ª´ context, kh√¥ng thi·∫øu bullet.
    - C√≤n l·∫°i -> LLM sinh t·ª± nhi√™n, c√≥ h·∫≠u ki·ªÉm & fallback n·∫øu thi·∫øu.
    """
    if llm is None:
        return pretty(context) if context else "Xin l·ªói, s·ªï tay ch∆∞a s·∫µn s√†ng."

    # G·ªçn context ƒë·ªÉ tr√°nh lo√£ng
    ctx = (context or "").strip()
    if len(ctx) > 3000:
        ctx = ctx[:3000]

    # ===== 1) Guardrail GI·ªú/TI·∫æT: t√°ch r√µ ti·∫øt c·ª• th·ªÉ v√† ca h·ªçc
    periods = extract_period_times(ctx)
    asked_period = question_targets_period(question)
    q_lower = question.lower()
    normalized_q = remove_vietnamese_diacritics(question)

    # N·∫øu h·ªèi v·ªÅ "bu·ªïi s√°ng"/"bu·ªïi chi·ªÅu" m√† KH√îNG c√≥ s·ªë ti·∫øt
    if asked_period is None and any(k in q_lower for k in ["v√†o h·ªçc", "b·∫Øt ƒë·∫ßu", "gi·ªù h·ªçc"]):
        if "s√°ng" in q_lower:
            return "Theo s·ªï tay: Bu·ªïi s√°ng b·∫Øt ƒë·∫ßu t·ª´ 07g00 nh√© "
        elif "chi·ªÅu" in q_lower:
            return "Theo s·ªï tay: Bu·ªïi chi·ªÅu b·∫Øt ƒë·∫ßu t·ª´ 13g30 nh√© "
        else:
            return "Theo s·ªï tay: Gi·ªù v√†o h·ªçc bu·ªïi s√°ng l√† 07g00 v√† bu·ªïi chi·ªÅu l√† 13g30 "

    # N·∫øu h·ªèi v·ªÅ ti·∫øt c·ª• th·ªÉ
    if asked_period is not None and asked_period in periods:
        st, ed = periods[asked_period]["start"], periods[asked_period]["end"]

        if any(k in q_lower for k in ["v√†o h·ªçc", "b·∫Øt ƒë·∫ßu", "m·∫•y gi·ªù", "gi·ªù h·ªçc"]):
            return f"Theo s·ªï tay: Ti·∫øt {asked_period} b·∫Øt ƒë·∫ßu l√∫c {st} v√† k·∫øt th√∫c l√∫c {ed}."
        elif "k·∫øt th√∫c" in q_lower or "h·∫øt" in q_lower:
            return f"Theo s·ªï tay: Ti·∫øt {asked_period} k·∫øt th√∫c l√∫c {ed} ."
        else:
            return f"Theo s·ªï tay: Ti·∫øt {asked_period} h·ªçc t·ª´ {st} ƒë·∫øn {ed}."


    # ===== 2) Guardrail H·ªåC B·ªîNG: √©p ƒë·ªß bullet n·∫øu l√† query ƒëi·ªÅu ki·ªán h·ªçc b·ªïng
    if is_scholarship_condition_query(question):
        bullets = extract_bullets(ctx)
        if bullets:
            # Cho ph√©p Gemma "n√≥i l·∫°i" cho m∆∞·ª£t nh∆∞ng v·∫´n ƒë·ªß √Ω:
            prompt_b = f"""
B·∫°n l√† chatbot h·ªçc v·ª• HCMUE. D·ª±a v√†o "Th√¥ng tin ch√≠nh th·ª©c" b√™n d∆∞·ªõi, h√£y tr√¨nh b√†y l·∫°i
**ƒë·∫ßy ƒë·ªß t·ª´ng ƒëi·ªÅu ki·ªán** d∆∞·ªõi d·∫°ng g·∫°ch ƒë·∫ßu d√≤ng, ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu, KH√îNG ƒë∆∞·ª£c b·ªè s√≥t.

Th√¥ng tin ch√≠nh th·ª©c:
---
{ctx}
---

C√¢u h·ªèi:
{question}

Y√™u c·∫ßu:
- Ch·ªâ d·ª±a v√†o vƒÉn b·∫£n ngu·ªìn, kh√¥ng th√™m d·ªØ ki·ªán.
- Gi·ªØ ƒë·∫ßy ƒë·ªß T·∫§T C·∫¢ ƒëi·ªÅu ki·ªán/bullet xu·∫•t hi·ªán trong ngu·ªìn, m·ªói √Ω m·ªôt d√≤ng.
- N·∫øu kh√¥ng li√™n quan, tr·∫£ l·ªùi: "S·ªï tay ch∆∞a ghi r√µ ph·∫ßn n√†y."

C√¢u tr·∫£ l·ªùi b·∫±ng g·∫°ch ƒë·∫ßu d√≤ng:
""".strip()

            try:
                ans = llm(
                    prompt_b,
                    max_new_tokens=180,
                    temperature=0.3,
                    top_p=0.9,
                    do_sample=True,
                    return_full_text=False
                )[0]["generated_text"].strip()

                # H·∫≠u ki·ªÉm: n·∫øu thi·∫øu bullet -> fallback tr√≠ch m√°y cho ch·∫Øc ch·∫Øn
                if (not has_bullet(ans)) or too_short(ans, 25):
                    ans = "T√≥m t·∫Øt ƒë·∫ßy ƒë·ªß theo s·ªï tay:\n" + "\n".join([f"- {b}" for b in bullets])
                return ans
            except Exception:
                # Fallback khi LLM l·ªói
                return "T√≥m t·∫Øt ƒë·∫ßy ƒë·ªß theo s·ªï tay:\n" + "\n".join([f"- {b}" for b in bullets])

    # ===== 3) C√°c truy v·∫•n kh√°c -> LLM + h·∫≠u ki·ªÉm chung
    prompt = f"""
B·∫°n l√† chatbot h·ªçc v·ª• c·ªßa Tr∆∞·ªùng ƒê·∫°i h·ªçc S∆∞ ph·∫°m TP.HCM (HCMUE).
Nhi·ªám v·ª• c·ªßa b·∫°n l√† gi√∫p sinh vi√™n hi·ªÉu r√µ th√¥ng tin trong s·ªï tay, b·∫±ng vƒÉn phong t·ª± nhi√™n, **r√µ r√†ng v√† ƒë·ªß √Ω**.

Th√¥ng tin ch√≠nh th·ª©c:
---
{ctx}
---

C√¢u h·ªèi:
{question}

H∆∞·ªõng d·∫´n tr·∫£ l·ªùi:
1) **Ch·ªâ d·ª±a v√†o ph·∫ßn "Th√¥ng tin ch√≠nh th·ª©c"**, kh√¥ng t·ª± suy lu·∫≠n.
2) Vi·∫øt ti·∫øng Vi·ªát t·ª± nhi√™n, d·ªÖ hi·ªÉu. 
3) N·∫øu c√≥ g·∫°ch ƒë·∫ßu d√≤ng/ƒëi·ªÅu ki·ªán trong ngu·ªìn, **li·ªát k√™ l·∫°i ƒë·∫ßy ƒë·ªß t·ª´ng d√≤ng**, kh√¥ng g·ªôp √Ω.
4) C√≥ th·ªÉ g·ªçn, nh∆∞ng **kh√¥ng ƒë∆∞·ª£c b·ªè s√≥t √Ω n√†o** trong ngu·ªìn.
5) Kh√¥ng t·∫°o c√¢u h·ªèi tr·∫Øc nghi·ªám. 
6) N·∫øu kh√¥ng c√≥ th√¥ng tin li√™n quan, tr·∫£ l·ªùi: "S·ªï tay ch∆∞a ghi r√µ ph·∫ßn n√†y."

C√¢u tr·∫£ l·ªùi:
""".strip()

    try:
        ans = llm(
            prompt,
            max_new_tokens=180,
            temperature=0.3,
            top_p=0.9,
            do_sample=True,
            return_full_text=False
        )[0]["generated_text"].strip()

        # L√†m s·∫°ch echo
        low = ans.lower()
        if "c√¢u tr·∫£ l·ªùi" in low:
            ans = ans.split(":", 1)[-1].strip()
        if "---" in ans:
            ans = ans.split("---")[-1].strip()

        # H·∫≠u ki·ªÉm: n·∫øu context c√≥ bullet m√† answer kh√¥ng c√≥/ qu√° ng·∫Øn -> regenerate nghi√™m ng·∫∑t
        ctx_has_bullets = has_bullet(ctx)
        need_retry = (ctx_has_bullets and (not has_bullet(ans))) or too_short(ans, 35)
        if need_retry:
            strict_prompt = prompt + "\n\n‚ö†Ô∏è Y√äU C·∫¶U B·ªî SUNG: Li·ªát k√™ ƒë·∫ßy ƒë·ªß t·ª´ng d√≤ng theo ngu·ªìn, m·ªói √Ω m·ªôt d√≤ng."
            ans2 = llm(
                strict_prompt,
                max_new_tokens=180,
                temperature=0.25,
                top_p=0.9,
                do_sample=False,
                return_full_text=False
            )[0]["generated_text"].strip()
            if len(ans2) > len(ans):
                ans = ans2

        # Fallback cu·ªëi: n·∫øu v·∫´n thi·∫øu bullet -> tr√≠ch m√°y t·ª´ context
        if ctx_has_bullets and (not has_bullet(ans)):
            bullets = extract_bullets(ctx)
            if bullets:
                ans = "T√≥m t·∫Øt ƒë·∫ßy ƒë·ªß theo s·ªï tay:\n" + "\n".join([f"- {b}" for b in bullets])

        # C·∫Øt b·ªõt n·∫øu qu√° d√†i
        if len(ans) > 1500:
            ans = ans[:1500].rstrip() + "‚Ä¶"

        if not ans:
            ans = "S·ªï tay ch∆∞a ghi r√µ ph·∫ßn n√†y."
        return ans

    except Exception as e:
        # Fallback an to√†n
        bullets = extract_bullets(ctx)
        if bullets:
            return "T√≥m t·∫Øt ƒë·∫ßy ƒë·ªß theo s·ªï tay:\n" + "\n".join([f"- {b}" for b in bullets])
        return f"‚ùå L·ªói sinh c√¢u tr·∫£ l·ªùi t·ª± nhi√™n: {e}"


# ========================================
# 8Ô∏è‚É£ H√ÄM X·ª¨ L√ù CH√çNH
# ========================================
def chatbot_query_json(question: str, top_k: int = 2) -> dict:
    question = question.strip()
    if not question:
        return {"type": "error", "message": "Vui l√≤ng nh·∫≠p c√¢u h·ªèi."}

    q_lower = question.lower()
    normalized_q = remove_vietnamese_diacritics(question)
    intent = classify_query_intent(question)

    course_keyword_hit = (
        bool(re.search(r"\bm√¥n\b", q_lower))
        or "h·ªçc ph·∫ßn" in q_lower
        or "m√¥n h·ªçc" in q_lower
        or "h·ªçc g√¨" in q_lower
        or "bao g·ªìm g√¨" in q_lower
        or ("n·ªôi dung" in q_lower and "m√¥n" in q_lower)
        or bool(re.search(r"\bmon\b", normalized_q))
        or "hoc phan" in normalized_q
        or "mon hoc" in normalized_q
        or "hoc gi" in normalized_q
        or "bao gom gi" in normalized_q
        or "noi dung mon" in normalized_q
    )
    is_course_intent = (intent == "COURSE") or course_keyword_hit

    schedule_tokens = [
        "buoi sang", "buoi chieu", "buoi toi", "lich hoc", "tiet",
        "ra choi", "nghi giua", "thu bay", "thu 7", "chu nhat",
        "gio vao hoc", "tan hoc", "vao toi"
    ]
    if any(token in normalized_q for token in schedule_tokens):
        is_course_intent = False

    greetings = [
        "ch√†o", "hi", "hello", "xin ch√†o", "yo", "alo", "hey",
        "c·∫≠u kho·∫ª kh√¥ng", "b·∫°n kh·ªèe kh√¥ng", "m√†y kh·ªèe kh√¥ng",
        "good morning", "good afternoon", "good evening"
    ]
    
    if any(re.search(rf"\b{g}\b", q_lower) for g in greetings):
        return {
            "type": "greeting",
            "natural_answer": "Ch√†o b·∫°n üëã! M√¨nh l√† chatbot HCMUE ‚Äì s·∫µn s√†ng gi√∫p b·∫°n tra c·ª©u th√¥ng tin h·ªçc v·ª• üí¨."
        }


    # ===== ∆ØU TI√äN: h·ªèi "Ti·∫øt X" =====
    asked_period = question_targets_period(question)
    periods = get_period_times()

    if (
        asked_period is not None
        and periods.get(asked_period)
        and not ("gio ra choi" in normalized_q or "nghi giua" in normalized_q)
    ):
        st = periods[asked_period]["start"]
        ed = periods[asked_period]["end"]
        ql = question.lower()
        if any(k in ql for k in ["v√†o h·ªçc", "b·∫Øt ƒë·∫ßu", "m·∫•y gi·ªù", "gi·ªù h·ªçc"]):
            return {"type": "schedule", "natural_answer": f"Theo s·ªï tay: Ti·∫øt {asked_period} b·∫Øt ƒë·∫ßu {st} v√† k·∫øt th√∫c {ed}."}
        elif "k·∫øt th√∫c" in ql or "h·∫øt" in ql:
            return {"type": "schedule", "natural_answer": f"Theo s·ªï tay: Ti·∫øt {asked_period} k·∫øt th√∫c {ed}."}
        else:
            return {"type": "schedule", "natural_answer": f"Theo s·ªï tay: Ti·∫øt {asked_period} h·ªçc t·ª´ {st} ƒë·∫øn {ed}."}

        # ===== ∆ØU TI√äN: h·ªèi 'bu·ªïi s√°ng/chi·ªÅu' =====
    morning_start, morning_end = "07g00", "11g30"
    afternoon_start, afternoon_end = "13g30", "17g30"
    end_keywords = ["ket thuc", "het", "tan"]
    start_keywords = ["bat dau", "vao hoc", "gio vao", "gio bat dau"]

    if "buoi sang" in normalized_q or "bu·ªïi s√°ng" in q_lower:
        if any(k in normalized_q for k in end_keywords):
            return {"type": "schedule", "natural_answer": f"Theo s·ªï tay: Bu·ªïi s√°ng k·∫øt th√∫c l√∫c {morning_end}."}
        if any(k in normalized_q for k in start_keywords):
            return {"type": "schedule", "natural_answer": f"Theo s·ªï tay: Bu·ªïi s√°ng b·∫Øt ƒë·∫ßu l√∫c {morning_start}."}
        return {"type": "schedule", "natural_answer": f"Theo s·ªï tay: Bu·ªïi s√°ng h·ªçc t·ª´ {morning_start} ƒë·∫øn {morning_end}."}
    elif "buoi chieu" in normalized_q or "bu·ªïi chi·ªÅu" in q_lower:
        if any(k in normalized_q for k in end_keywords):
            return {"type": "schedule", "natural_answer": f"Theo s·ªï tay: Bu·ªïi chi·ªÅu k·∫øt th√∫c l√∫c {afternoon_end}."}
        if any(k in normalized_q for k in start_keywords):
            return {"type": "schedule", "natural_answer": f"Theo s·ªï tay: Bu·ªïi chi·ªÅu b·∫Øt ƒë·∫ßu l√∫c {afternoon_start}."}
        return {"type": "schedule", "natural_answer": f"Theo s·ªï tay: Bu·ªïi chi·ªÅu h·ªçc t·ª´ {afternoon_start} ƒë·∫øn {afternoon_end}."}
    elif ("gi·ªù v√†o h·ªçc" in q_lower or "v√†o h·ªçc" in q_lower) and not any(k in q_lower for k in ["t·ªët nghi·ªáp", "hoc bong", "r√®n luy·ªán", "tot nghiep", "ren luyen"]):
        # fallback chung ch·ªâ khi th·∫≠t s·ª± n√≥i v·ªÅ gi·ªù h·ªçc
        return {
            "type": "schedule",
            "natural_answer": (
                f"Theo s·ªï tay: Bu·ªïi s√°ng h·ªçc t·ª´ {morning_start} ƒë·∫øn {morning_end} v√† bu·ªïi chi·ªÅu h·ªçc t·ª´ {afternoon_start} ƒë·∫øn {afternoon_end}."
            )
        }

    if (
        "buoi toi" in normalized_q
        or ("vao toi" in normalized_q and "hoc" in normalized_q)
        or ("hoc toi" in normalized_q)
    ):
        return {
            "type": "schedule",
            "natural_answer": "Theo s·ªï tay: L·ªãch h·ªçc ch√≠nh kh√≥a ch·ªâ c√≥ ca s√°ng (07g00) v√† ca chi·ªÅu (13g30), kh√¥ng c√≥ ca h·ªçc bu·ªïi t·ªëi."
        }

    if "chu nhat" in normalized_q and "hoc" in normalized_q:
        return {"type": "schedule", "natural_answer": "Theo s·ªï tay: Nh√† tr∆∞·ªùng kh√¥ng s·∫Øp l·ªõp ch√≠nh kh√≥a v√†o Ch·ªß nh·∫≠t."}

    if ("thu bay" in normalized_q or "thu 7" in normalized_q) and any(tok in normalized_q for tok in ["hoc", "lop", "chinh khoa"]):
        return {"type": "schedule", "natural_answer": "Theo s·ªï tay: Nh√† tr∆∞·ªùng kh√¥ng s·∫Øp l·ªõp ch√≠nh kh√≥a v√†o Th·ª© b·∫£y."}

    if is_minimum_credit_query(normalized_q):
        return {
            "type": "natural_table",
            "natural_answer": (
                "Theo s·ªï tay: ƒê·ªÉ ƒë∆∞·ª£c x√©t h·ªçc b·ªïng khuy·∫øn kh√≠ch, sinh vi√™n c·∫ßn t√≠ch l≈©y t·ªëi thi·ªÉu 15 t√≠n ch·ªâ trong h·ªçc k·ª≥ x√©t h·ªçc b·ªïng (kh√¥ng t√≠nh t√≠n ch·ªâ tr·∫£ n·ª£/c·∫£i thi·ªán); ƒë·ªìng th·ªùi ph·∫£i ho√†n th√†nh √≠t nh·∫•t 2/3 s·ªë t√≠n ch·ªâ ƒëƒÉng k√Ω trong h·ªçc k·ª≥ ƒë√≥."
            ),
        }

    if any(phrase in normalized_q for phrase in ["tot nghiep", "totnghiep", "xet tot nghiep", "cong nhan tot nghiep"]):
        graduation_answer = (
            "Theo s·ªï tay: ƒêi·ªÅu ki·ªán x√©t t·ªët nghi·ªáp g·ªìm:\n"
            "- T√≠ch l≈©y ƒë·ªß h·ªçc ph·∫ßn, s·ªë t√≠n ch·ªâ v√† ho√†n th√†nh c√°c n·ªôi dung b·∫Øt bu·ªôc kh√°c theo y√™u c·∫ßu c·ªßa CTƒêT, ƒë·∫°t chu·∫©n ƒë·∫ßu ra c·ªßa CTƒêT.\n"
            "- ƒêi·ªÉm trung b√¨nh t√≠ch l≈©y c·ªßa to√†n kh√≥a h·ªçc ƒë·∫°t t·ª´ trung b√¨nh tr·ªü l√™n.\n"
            "- T·∫°i th·ªùi ƒëi·ªÉm x√©t t·ªët nghi·ªáp kh√¥ng b·ªã truy c·ª©u tr√°ch nhi·ªám h√¨nh s·ª± ho·∫∑c kh√¥ng ƒëang trong th·ªùi gian b·ªã k·ª∑ lu·∫≠t ·ªü m·ª©c ƒë√¨nh ch·ªâ h·ªçc t·∫≠p."
        )
        return {"type": "natural_table", "natural_answer": graduation_answer}

    if "gio ra choi" in normalized_q or "nghi giua" in normalized_q:
        period_numbers = [int(p) for p in re.findall(r"tiet\s*(\d+)", normalized_q)]
        if len(period_numbers) < 2:
            all_numbers = [int(p) for p in re.findall(r"\b(\d+)\b", normalized_q)]
            for num in all_numbers:
                if num not in period_numbers:
                    period_numbers.append(num)
                if len(period_numbers) >= 2:
                    break
        if len(period_numbers) >= 2:
            period_numbers = sorted(period_numbers)[:2]
            duration = compute_break_duration(period_numbers[0], period_numbers[1])
            duration_text = format_duration(duration)
            if duration == 0:
                return {
                    "type": "schedule",
                    "natural_answer": (
                        f"Theo s·ªï tay: Gi·ªØa ti·∫øt {period_numbers[0]} v√† ti·∫øt {period_numbers[1]} kh√¥ng c√≥ th·ªùi gian ngh·ªâ ri√™ng."
                    )
                }
            if duration_text:
                start_label = periods.get(period_numbers[0], {}).get("end") if periods else None
                end_label = periods.get(period_numbers[1], {}).get("start") if periods else None
                window_text = ""
                if start_label and end_label:
                    window_text = f" (t·ª´ {start_label} ƒë·∫øn {end_label})"
                return {
                    "type": "schedule",
                    "natural_answer": (
                        f"Theo s·ªï tay: Ngh·ªâ gi·ªØa ti·∫øt {period_numbers[0]} v√† ti·∫øt {period_numbers[1]}"
                        f" k√©o d√†i {duration_text}{window_text}."
                    )
                }
        return {"type": "schedule", "natural_answer": "Theo s·ªï tay: Th·ªùi gian ngh·ªâ gi·ªØa c√°c ti·∫øt ch∆∞a ƒë∆∞·ª£c ghi r√µ."}

    if "hoc bong" in normalized_q:
        if is_scholarship_no_debt_query(normalized_q):
            return {
                "type": "natural_table",
                "natural_answer": (
                    "Theo s·ªï tay: Khi x√©t h·ªçc b·ªïng, t·∫•t c·∫£ t√≠n ch·ªâ ƒëƒÉng k√Ω trong h·ªçc k·ª≥ ph·∫£i ƒë·∫°t n√™n sinh vi√™n kh√¥ng ƒë∆∞·ª£c n·ª£ m√¥n trong h·ªçc k·ª≥ x√©t h·ªçc b·ªïng."
                ),
            }

        if is_poor_conduct_query(normalized_q):
            return {
                "type": "natural_table",
                "natural_answer": (
                    "Theo s·ªï tay: Ti√™u chu·∫©n h·ªçc b·ªïng y√™u c·∫ßu ƒëi·ªÉm r√®n luy·ªán t·ª´ lo·∫°i Kh√° (t·ª´ 65 ƒëi·ªÉm) tr·ªü l√™n, n√™n ƒëi·ªÉm r√®n luy·ªán Trung b√¨nh kh√¥ng ƒë·ªß ƒëi·ªÅu ki·ªán."
                ),
            }

        level_key = detect_scholarship_level(normalized_q)
        if level_key:
            label, combos = get_scholarship_level_details(level_key)
            visible_label = label or SCHOLARSHIP_LEVEL_LABELS.get(level_key)
            if not combos:
                if visible_label:
                    return {
                        "type": "natural_table",
                        "natural_answer": (
                            f"Theo s·ªï tay: Kh√¥ng c√≥ h·ªçc b·ªïng khuy·∫øn kh√≠ch h·ªçc t·∫≠p d√†nh cho m·ª©c {visible_label.lower()}."
                        ),
                    }
                return {
                    "type": "natural_table",
                    "natural_answer": "Theo s·ªï tay: Kh√¥ng t√¨m th·∫•y th√¥ng tin h·ªçc b·ªïng ph√π h·ª£p v·ªõi m·ª©c b·∫°n h·ªèi.",
                }
            if combos:
                academic_only = ("diem hoc tap" in normalized_q and "ren luyen" not in normalized_q and "diem trung binh" not in normalized_q)
                header = f"Theo s·ªï tay: Ti√™u chu·∫©n h·ªçc b·ªïng {visible_label} g·ªìm:" if visible_label else "Theo s·ªï tay:"
                if academic_only:
                    academic_requirements = []
                    seen = set()
                    for combo in combos:
                        desc = combo.get("academic_desc")
                        key = desc or ""
                        if desc and key not in seen:
                            academic_requirements.append(desc)
                            seen.add(key)
                    if academic_requirements:
                        natural = f"Theo s·ªï tay: ƒêi·ªÉm h·ªçc t·∫≠p t·ªëi thi·ªÉu cho h·ªçc b·ªïng {visible_label} g·ªìm:\n" + "\n".join(f"- {req}" for req in academic_requirements)
                        return {"type": "natural_table", "natural_answer": natural}
                detail_lines = []
                seen_combo = set()
                for combo in combos:
                    parts = [p for p in [combo.get("academic_desc"), combo.get("conduct_desc")] if p]
                    if parts:
                        text = "; ".join(parts)
                        if text not in seen_combo:
                            detail_lines.append(text)
                            seen_combo.add(text)
                if detail_lines:
                    natural = header + "\n" + "\n".join(f"- {line}" for line in detail_lines)
                    return {"type": "natural_table", "natural_answer": natural}

        if is_scholarship_condition_query(question) or "khuyen khich" in normalized_q:
            return {"type": "natural_table", "natural_answer": GENERAL_SCHOLARSHIP_CONDITIONS}

        if SCHOLARSHIP_LOOKUP:
            summary = []
            for key, item in SCHOLARSHIP_LOOKUP.items():
                label, combos = get_scholarship_level_details(key)
                if label and combos:
                    temp_lines = []
                    for combo in combos:
                        parts = [p for p in [combo.get("academic_desc"), combo.get("conduct_desc")] if p]
                        if parts:
                            temp_lines.append("; ".join(parts))
                    for line in temp_lines:
                        summary.append(f"{label}: {line}")
            if summary:
                natural = "Theo s·ªï tay: Ti√™u chu·∫©n h·ªçc b·ªïng g·ªìm:\n" + "\n".join(f"- {line}" for line in summary)
                return {"type": "natural_table", "natural_answer": natural}

    if "diem ren luyen" in normalized_q:
        if "toi da" in normalized_q:
            return {"type": "natural_table", "natural_answer": "Theo s·ªï tay: ƒêi·ªÉm r√®n luy·ªán t·ªëi ƒëa l√† 100 ƒëi·ªÉm."}
        score_matches = re.findall(r"\d+(?:[.,]\d+)?", question)
        if score_matches:
            try:
                score_value = float(score_matches[0].replace(",", "."))
            except ValueError:
                score_value = None
            rank_info = determine_training_rank(score_value)
            if rank_info:
                score_text = format_decimal(score_value) if score_value is not None else score_matches[0]
                return {
                    "type": "natural_table",
                    "natural_answer": (
                        f"Theo s·ªï tay: ƒêi·ªÉm r√®n luy·ªán {score_text} x·∫øp lo·∫°i {rank_info['label']} ({rank_info['range']})."
                    ),
                }
        for key, item in TRAINING_RANK_LOOKUP.items():
            if key in normalized_q:
                return {
                    "type": "natural_table",
                    "natural_answer": f"Theo s·ªï tay: ƒêi·ªÉm r√®n luy·ªán x·∫øp lo·∫°i {item['label']} khi ƒë·∫°t {item['range']}."
                }
        if TRAINING_RANK_LOOKUP:
            overview = "\n".join(f"- {item['label']}: {item['range']}" for item in TRAINING_RANK_LOOKUP.values())
            return {"type": "natural_table", "natural_answer": "Theo s·ªï tay: X·∫øp lo·∫°i ƒëi·ªÉm r√®n luy·ªán g·ªìm:\n" + overview}

    if is_general_pass_requirement_query(normalized_q):
        return handle_pass_requirement_query(question, normalized_q)

    informal_phrases = [
        "may biet",
        "cho tao",
        "may co biet",
        "cho tui",
        "lam bai giup",
        "lam bai gium",
        "lam bai ho",
        "lam bai giup tao",
        "lam bai giup tui",
    ]
    if any(phrase in normalized_q for phrase in informal_phrases):
        return {"‚ùå C√¢u h·ªèi ch∆∞a h·ª£p l·ªá ho·∫∑c kh√¥ng r√µ r√†ng. B·∫°n th·ª≠ h·ªèi l·∫°i c·ª• th·ªÉ h∆°n nh√©!"}
    
    requested_class_key = detect_academic_class_key(normalized_q)
    if requested_class_key:
        row = ACADEMIC_CLASS_LOOKUP.get(requested_class_key)
        if row:
            label = str(row.get("X·∫øp lo·∫°i", "")).strip()
            range_text = str(row.get("Thang ƒëi·ªÉm 4", "")).strip()
            if label and range_text:
                return {
                    "type": "natural_table",
                    "natural_answer": f"Theo s·ªï tay: X·∫øp lo·∫°i {label} khi ƒëi·ªÉm trung b√¨nh thang 4 {range_text}."
                }
            if label:
                return {
                    "type": "natural_table",
                    "natural_answer": f"Theo s·ªï tay: X·∫øp lo·∫°i {label} ƒë∆∞·ª£c quy ƒë·ªãnh trong s·ªï tay ƒë√†o t·∫°o."
                }

    matched_class_rows = []
    for key, row in ACADEMIC_CLASS_LOOKUP.items():
        if key and re.search(rf"\bxep\s+loai\s+{key}\b", normalized_q):
            matched_class_rows.append(row)

    if matched_class_rows:
        answers = []
        for row in matched_class_rows:
            label = str(row.get("X·∫øp lo·∫°i", "")).strip()
            range_text = str(row.get("Thang ƒëi·ªÉm 4", "")).strip()
            if label and range_text:
                answers.append(f"X·∫øp lo·∫°i {label} khi ƒëi·ªÉm trung b√¨nh thang 4 {range_text}.")
            elif label:
                answers.append(f"X·∫øp lo·∫°i {label} ƒë∆∞·ª£c x√°c ƒë·ªãnh trong s·ªï tay.")
        if answers:
            if len(answers) == 1:
                natural = f"Theo s·ªï tay: {answers[0]}"
            else:
                natural = "Theo s·ªï tay:\n" + "\n".join(f"- {ans}" for ans in answers)
            return {"type": "natural_table", "natural_answer": natural}

    grade = None
    if not is_course_intent:
        grade = extract_grade_token(question)

        if grade is None and "diem chu" in normalized_q:
            if any(kw in normalized_q for kw in ["dat toi thieu", "duoc xem la dat", "dat duoc", "dat toi"]):
                passing_list = ", ".join(PASSING_GRADE_ASC_ORDER)
                return {
                    "type": "natural_table",
                    "natural_answer": f"Theo s·ªï tay: ƒêi·ªÉm ch·ªØ t·ª´ D tr·ªü l√™n ({passing_list}) ƒë∆∞·ª£c xem l√† ƒë·∫°t t·ªëi thi·ªÉu."
                }
            if any(kw in normalized_q for kw in ["rot", "truot", "khong dat", "khong du dieu kien"]):
                failing_list = ", ".join(FAILING_GRADE_ORDER)
                return {
                    "type": "natural_table",
                    "natural_answer": f"Theo s·ªï tay: ƒêi·ªÉm ch·ªØ {failing_list.replace(', ', ' ho·∫∑c ')} b·ªã xem l√† r·ªõt (kh√¥ng ƒë·∫°t)."
                }

        if grade is None and "thang diem chu" in normalized_q and "thang diem 10" in normalized_q:
            for t in tables:
                if t.get("type") == "thang_diem_10_chu":
                    df = pd.DataFrame(t["data"])
                    rows = [f"- {row['Thang ƒëi·ªÉm ch·ªØ']}: {row['Thang ƒëi·ªÉm 10']}" for row in df.to_dict("records")]
                    natural = "Theo s·ªï tay: Quy ƒë·ªïi thang ƒëi·ªÉm 10 sang thang ƒëi·ªÉm ch·ªØ:\n" + "\n".join(rows)
                    return {"type": "natural_table", "natural_answer": natural}

        if grade is None and "thang diem chu" in normalized_q and ("thang diem 4" in normalized_q or re.search(r"\bthang\s*4\b", normalized_q)):
            for t in tables:
                if t.get("type") == "thang_diem_4":
                    df = pd.DataFrame(t["data"])
                    rows = [f"- {row['Thang ƒëi·ªÉm ch·ªØ']}: {row['Thang ƒëi·ªÉm 4']}" for row in df.to_dict("records")]
                    natural = "Theo s·ªï tay: Quy ƒë·ªïi thang ƒëi·ªÉm 4 sang thang ƒëi·ªÉm ch·ªØ:\n" + "\n".join(rows)
                    return {"type": "natural_table", "natural_answer": natural}

        if "thang diem 4" in normalized_q and any(kw in normalized_q for kw in ["loai hoc luc", "xep loai hoc luc", "xep loai"]):
            for t in tables:
                if t.get("type") == "xep_loai_hoc_luc":
                    df = pd.DataFrame(t["data"])
                    rows = [f"- {row['X·∫øp lo·∫°i']}: {row['Thang ƒëi·ªÉm 4']}" for row in df.to_dict("records")]
                    natural = "Theo s·ªï tay: Thang ƒëi·ªÉm 4 ƒë∆∞·ª£c x·∫øp lo·∫°i h·ªçc l·ª±c nh∆∞ sau:\n" + "\n".join(rows)
                    return {"type": "natural_table", "natural_answer": natural}

        if grade:
            if "dat" in normalized_q:
                info = GRADE_LOOKUP.get(grade, {})
                thang10 = info.get("thang_10")
                thang4 = info.get("thang_4")
                if grade in PASSING_GRADES:
                    detail = []
                    if thang4:
                        detail.append(f"thang 4: {thang4}")
                    if thang10:
                        detail.append(f"thang 10: {thang10}")
                    detail_text = ", ".join(detail)
                    postfix = f" ({detail_text})" if detail_text else ""
                    return {
                        "type": "natural_table",
                        "natural_answer": f"Theo s·ªï tay: ƒêi·ªÉm ch·ªØ t·ª´ D tr·ªü l√™n ƒë∆∞·ª£c xem l√† ƒë·∫°t. {grade} thu·ªôc nh√≥m ƒë·∫°t{postfix}."
                    }
                if grade in FAILING_GRADES:
                    detail = []
                    if thang4:
                        detail.append(f"thang 4 {thang4}")
                    if thang10:
                        detail.append(f"thang 10 {thang10}")
                    detail_text = ", ".join(detail)
                    postfix = f" ({detail_text})" if detail_text else ""
                    return {
                        "type": "natural_table",
                        "natural_answer": f"Theo s·ªï tay: ƒêi·ªÉm ch·ªØ F ho·∫∑c F+ l√† kh√¥ng ƒë·∫°t. {grade} kh√¥ng ƒë∆∞·ª£c xem l√† ƒë·∫°t{postfix}."
                    }

            prefer_grade_thang4 = ("thang diem 4" in normalized_q) or ("thang 4" in normalized_q)
            prefer_grade_thang10 = ("thang diem 10" in normalized_q) or ("thang 10" in normalized_q)

            info = GRADE_LOOKUP.get(grade)
            if info:
                thang4 = info.get("thang_4")
                thang10 = info.get("thang_10")

                if prefer_grade_thang4 and thang4:
                    return {
                        "type": "natural_table",
                        "natural_answer": f"Theo s·ªï tay: {grade} t∆∞∆°ng ƒë∆∞∆°ng {thang4} ƒëi·ªÉm thang 4."
                    }

                if prefer_grade_thang10 and thang10:
                    return {
                        "type": "natural_table",
                        "natural_answer": f"Theo s·ªï tay: {grade} t∆∞∆°ng ƒë∆∞∆°ng {thang10} ƒëi·ªÉm thang 10."
                    }

                # Kh√¥ng ch·ªâ r√µ thang ‚Üí ∆∞u ti√™n thang 10 n·∫øu c√≥, ng∆∞·ª£c l·∫°i tr·∫£ thang 4
                if thang10:
                    return {
                        "type": "natural_table",
                        "natural_answer": f"Theo s·ªï tay: {grade} t∆∞∆°ng ƒë∆∞∆°ng {thang10} ƒëi·ªÉm thang 10."
                    }
                if thang4:
                    return {
                        "type": "natural_table",
                        "natural_answer": f"Theo s·ªï tay: {grade} t∆∞∆°ng ƒë∆∞∆°ng {thang4} ƒëi·ªÉm thang 4."
                    }

        # =====  NG∆Ø·ª¢C L·∫†I: ng∆∞·ªùi d√πng h·ªèi t·ª´ ƒëi·ªÉm s·ªë sang ch·ªØ =====
        num_match = re.search(r"(\d+[.,]?\d*)", q_lower)

        if num_match:
            num_str = num_match.group(1).replace(",", ".")
            try:
                num_val = float(num_str)
            except:
                num_val = None

            if num_val is not None:
                prefers_thang4 = ("thang diem 4" in normalized_q) or ("thang 4" in normalized_q) or (num_val <= 4 and "thang diem 10" not in normalized_q)
                prefers_thang10 = ("thang diem 10" in normalized_q) or ("thang 10" in normalized_q) or (num_val > 4 and "thang diem 4" not in normalized_q)

                if prefers_thang4:
                    letter = grade_letter_from_score(num_val, 4)
                    if letter:
                        return {
                            "type": "natural_table",
                            "natural_answer": (
                                f"Theo s·ªï tay: {format_score_text(num_val)} ƒëi·ªÉm thang 4 t∆∞∆°ng ƒë∆∞∆°ng ƒëi·ªÉm ch·ªØ {letter}."
                            ),
                        }
                    return {
                        "type": "natural_table",
                        "natural_answer": (
                            f"Theo s·ªï tay: {format_score_text(num_val)} ƒëi·ªÉm thang 4 ti·ªám c·∫≠n ƒëi·ªÉm ch·ªØ D (m·ªëc ƒë·∫°t t·ªëi thi·ªÉu)."
                        ),
                    }

                if prefers_thang10:
                    letter = grade_letter_from_score(num_val, 10)
                    if letter:
                        return {
                            "type": "natural_table",
                            "natural_answer": (
                                f"Theo s·ªï tay: {format_score_text(num_val)} ƒëi·ªÉm thang 10 t∆∞∆°ng ƒë∆∞∆°ng ƒëi·ªÉm ch·ªØ {letter}."
                            ),
                        }
                    return {
                        "type": "natural_table",
                        "natural_answer": (
                            f"Theo s·ªï tay: {format_score_text(num_val)} ƒëi·ªÉm thang 10 kh√¥ng kh·ªõp v·ªõi thang quy ƒë·ªïi hi·ªán c√≥."
                        ),
                    }

    if is_course_intent:
        course = find_course_by_fuzzy_match(question)
        if course:
            return {"type": "course", "data": course}

    tb = find_table_by_keyword(question)
    if tb:
        return {"type": "table", "data": tb}

    # ===== 4) M·∫∑c ƒë·ªãnh: RAG chung + fallback t·ªët nghi·ªáp =====
    try:
        if any(k in q_lower for k in ["t·ªët nghi·ªáp", "tot nghiep", "x√©t t·ªët nghi·ªáp", "cong nhan tot nghiep"]):
            vec_results = vector_search(question, top_k=1)
        else:
            vec_results = vector_search(question, top_k=max(top_k, 2))

        if not vec_results:
            return {"type": "vector_search", "results": [], "message": "Kh√¥ng t√¨m th·∫•y th√¥ng tin."}

        sims = [1 - v["distance"] for v in vec_results if v.get("distance") is not None]
        best_sim = sims[0] if sims else 0.0
        threshold = 0.7

        if best_sim < threshold:
            # ‚úÖ Fallback ƒë·∫∑c bi·ªát cho 't·ªët nghi·ªáp'
            if any(k in q_lower.replace(" ", "") for k in ["t·ªëtnghi·ªáp", "totnghiep", "x√©tt·ªëtnghi·ªáp", "congnhantotnghiep"]):
                retry_q = "ƒëi·ªÅu ki·ªán c√¥ng nh·∫≠n t·ªët nghi·ªáp, x√©t t·ªët nghi·ªáp, quy ƒë·ªãnh c√¥ng nh·∫≠n t·ªët nghi·ªáp"
                vec_results = vector_search(retry_q, top_k=1)
                sims = [1 - v["distance"] for v in vec_results if v.get("distance") is not None]
                if sims and sims[0] >= 0.6:  # h·∫° nh·∫π threshold ri√™ng cho t·ªët nghi·ªáp
                    combined_context = "\n".join([v["chunk"] for v in vec_results])
                    natural = generate_natural_answer(question, combined_context)
                    return {"type": "vector_search", "results": vec_results, "natural_answer": natural}
            # N·∫øu kh√¥ng ph·∫£i nh√≥m 't·ªët nghi·ªáp' ho·∫∑c retry v·∫´n th·∫•p ‚Üí invalid
            return {"type": "invalid_query", "message": "‚ùå C√¢u h·ªèi ch∆∞a h·ª£p l·ªá ho·∫∑c kh√¥ng r√µ r√†ng. B·∫°n th·ª≠ h·ªèi l·∫°i c·ª• th·ªÉ h∆°n nh√©!"}

        combined_context = "\n".join([v["chunk"] for v in vec_results])
        natural = generate_natural_answer(question, combined_context)
        return {"type": "vector_search", "results": vec_results, "natural_answer": natural}

    except Exception as e:
        return {"type": "error", "message": f"L·ªói truy v·∫•n vector: {e}"}



# ========================================
# 9Ô∏è‚É£ API ENDPOINTS
# ========================================
class QueryRequest(BaseModel):
    query: str
    top_k: int = 2

@app.post("/query")
async def process_query(request: QueryRequest):
    if SYSTEM_STATUS != "OK":
        raise HTTPException(status_code=503, detail={"error": "H·ªá th·ªëng ch∆∞a s·∫µn s√†ng", "status": SYSTEM_STATUS})
    return chatbot_query_json(request.query, request.top_k)

@app.get("/")
def home():
    return {"message": "‚úÖ Chatbot HCMUE RAG API ƒëang ho·∫°t ƒë·ªông!", "status": SYSTEM_STATUS, "model": MODEL_NAME}